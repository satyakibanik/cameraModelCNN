{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/HTC-1-M7/*.jpg'\n",
    "\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '1'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'w')\n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/iPhone-4s/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '2'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/iPhone-6/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '3'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')\n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/LG Nexus 5x/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '4'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Motorola Droid Maxx/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '5'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Motorola Nexus 6/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '6'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Motorola X/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '7'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Samsung Galaxy Note 4/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '8'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Samsung Galaxy S4/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '9'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Sony Nex 7/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '10'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for shuffle the image \n",
    "import random\n",
    "# read the csv file \n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "#create a new csv file as shuffled_labels.csv where the image address will be shuffled\n",
    "shuffled_labels = 'E:/IP/Labels/shuffled_labels.csv'\n",
    "\n",
    "labelfile = open(labels, \"r\")\n",
    "lines = labelfile.readlines()\n",
    "labelfile.close()\n",
    "random.shuffle(lines)\n",
    "# create shuffle csv\n",
    "shufflefile = open(shuffled_labels, \"w\") \n",
    "shufflefile.writelines(lines)\n",
    "shufflefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "labels = 'E:/IP/Labels/shuffled_labels.csv'\n",
    "npzfile = 'E:/IP/Labels/labels.npz'\n",
    "#create npz hiden  file \n",
    "df = pandas.read_csv(labels)\n",
    "\n",
    "rows = df.iterrows()\n",
    "\n",
    "X_temp = []\n",
    "Y_temp = []\n",
    "\n",
    "for row in rows:\n",
    "    image = row[1][0]\n",
    "    img = cv2.imread(image)\n",
    "    #npimg = np.asarray(img)\n",
    "    #npimg = npimg/255\n",
    "    (b, g, r)=cv2.split(img)\n",
    "    img=cv2.merge([r,g,b])\n",
    "    imageClass = row[1][1]\n",
    "    X_temp.append(img)\n",
    "    Y_temp.append(imageClass)\n",
    "\n",
    "       \n",
    "# one hot encoding to represent one num as a array like 2= [0 1 0] in a 3 class cnn model     \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_temp)\n",
    "encoded_Y = encoder.transform(Y_temp)\n",
    "Y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "np.savez(npzfile, X_train=X_temp,Y_train=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, time, datetime\n",
    "import numpy as np\n",
    "\n",
    "sd = 7\n",
    "np.random.seed(sd)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(sd)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(),'saved_models_keras')\n",
    "model_name = 'SPcup_trained_model.h5'\n",
    "\n",
    "\n",
    "npzfile = 'E:/IP/Labels/labels.npz'\n",
    "\n",
    "dataset =  np.load(npzfile)\n",
    "x_train = dataset['X_train']\n",
    "y_train = dataset['Y_train']\n",
    "\n",
    "x = x_train\n",
    "\n",
    "num_classes = 10\n",
    "eps = 1e-6\n",
    "dropout_rate_layers = 0.2\n",
    "dropout_rate_dense = 0.05\n",
    "maxpool_size = 2\n",
    "kernel_size = 5\n",
    "stride = 1\n",
    "learning_rate = 0.0003\n",
    "lr_decay = 0.0\n",
    "bias = False\n",
    "val_split = 0.3\n",
    "batch_size = 16\n",
    "nepoch = 20\n",
    "l2reg_conv = 0.01\n",
    "l2reg_dense = 0.005\n",
    "maxnorm = 10\n",
    "run_idx = 11\n",
    "\n",
    "\n",
    "# CNN MODEL STARTS!\n",
    "\n",
    "def CNN_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #layer 1\n",
    "    model.add(Conv2D(32, kernel_size=kernel_size-1, \n",
    "                strides=stride,\n",
    "                padding='valid',\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=bias,\n",
    "                input_shape=(256,256,3)))\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    #layer 2\n",
    "    model.add(Conv2D(64,kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding='valid',\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=bias \n",
    "               ))\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    \n",
    "    #layer 3\n",
    "    model.add(Conv2D(128,kernel_size=kernel_size, \n",
    "                #strides=stride,\n",
    "                padding='valid',\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=None ))\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    #layer 4\n",
    "    model.add(Conv2D(128,kernel_size=kernel_size, \n",
    "                #strides=stride,\n",
    "                padding='valid',\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=None)\n",
    "             )\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    # now the fully connected layers!\n",
    "    \n",
    "    # DENSE layer 1\n",
    "    model.add(Dense(128, activation='relu',\n",
    "                   use_bias=bias,\n",
    "                   kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                   kernel_regularizer=regularizers.l2(l2reg_dense),\n",
    "                   kernel_constraint=max_norm(maxnorm)\n",
    "                  ))\n",
    "    #model.add(Dropout(rate=dropout_rate_dense, seed=sd))\n",
    "    \n",
    "    # DENSE layer 2\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    adam = Adam(lr=learning_rate,decay=lr_decay)\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= adam , metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "# CNN MODEL ENDS!\n",
    "\n",
    "model = CNN_model()\n",
    "\n",
    "model.summary()\n",
    "# check will store the best model weight which will monitor the validation accuracy not train accuracy\n",
    "file_path = 'best' + '_' + str(run_idx) + '.hdf5'\n",
    "\n",
    "#setting up checkpoint save directory/ log names\n",
    "checkpoint_path=os.path.join(os.path.join(os.getcwd(),'tmp'),str(date.today()))\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_name=checkpoint_path+'/'+str(run_idx)+'weights.{epoch:02d}-{val_acc:.4f}.hdf5'\n",
    "log_name='logs'+ '_' + str(date.today()) + '_' + str(run_idx)\n",
    "\n",
    "#checkpoints!\n",
    "modelcheck=ModelCheckpoint(filepath=checkpoint_name,monitor='val_catagorical_acc', verbose=1,save_best_only=True,mode='max')\n",
    "tensorbd=TensorBoard(log_dir='./logs/'+log_name,batch_size=batch_size,write_images=True)\n",
    "csv_logger = CSVLogger('./logs/training_'+log_name+'.log',separator=',', append=True )\n",
    "#modelcheck  = ModelCheckpoint(file_path, monitor = 'val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoints = [modelcheck, tensorbd, csv_logger]\n",
    "\n",
    "#fitting model to for validation\n",
    "training = model.fit(x, y_train, \n",
    "                     validation_split = val_split,\n",
    "                     epochs=nepoch, \n",
    "                     batch_size=batch_size, \n",
    "                     verbose=2, #verbose=1 can result in slower training time.\n",
    "                     shuffle=True,\n",
    "                     callbacks = checkpoints)\n",
    "print(training)\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# show the loss and accuracy\n",
    "loss = training.history['loss']\n",
    "val_loss = training.history['val_loss']\n",
    "acc = training.history['acc']\n",
    "val_acc = training.history['val_acc']\n",
    "\n",
    "# loss plot\n",
    "tra = plt.plot(loss)\n",
    "val = plt.plot(val_loss, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# accuracy plot\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOHEL'S FIRST CODE FOR TESTING.\n",
    "\n",
    "#predict the class\n",
    "y = model.predict_classes(X)\n",
    "# y is a one dim array we have to convert it to a classs num like 3,4 \n",
    "classno = np.ndarray.tolist(y)\n",
    "\n",
    "dict = {0: 'LG', 1:'moto maxx', 2: 'sony,}\n",
    "objectClass = dict[classno[0]]\n",
    "print(objectClass)\n",
    "print(y)\n",
    "        \n",
    "#yPred = model.predict_classes(x_valid,batch_size=32,verbose=1)\n",
    "\n",
    "#np.savetxt('mnist_output.csv', np.c_[range(1,len(yPred)+1),yPred], delimiter=',', header = 'ImageId,Label', comments = '', fmt='%d')\n",
    "\n",
    "#font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#cv2.putText(img, objectClass,(50,50), font, 2, (200,255,0), 5, cv2.LINE_AA)\n",
    "#cv2.imshow('Prediction',img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOT FROM THIS LINK: https://goo.gl/kRPp9p\n",
    "\n",
    "# load the best model\n",
    "model = load_model(file_path)\n",
    "\n",
    "# load test_data\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# prediction\n",
    "pred = model.predict(test_features, batch_size=16, \n",
    "                       verbose=1)\n",
    "\n",
    "# convert predicions from categorical back to 0...9 digits\n",
    "pred_digits = np.argmax(pred, axis=1)\n",
    "\n",
    "submission = pandas.DataFrame({'Label': pred_digits})\n",
    "submission.index += 1\n",
    "submission.index.name = \"ImageId\"\n",
    "\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KAGGLE SUBMISSION TYPE.\n",
    "\n",
    "############## generating X_test ##############\n",
    "import os\n",
    "\n",
    "list_paths = []\n",
    "for subdir, dirs, files in os.walk(\"E:\\IP\"):\n",
    "    for file in files:\n",
    "        #print(os.path.join(subdir, file))\n",
    "        filepath = subdir + os.sep + file\n",
    "        list_paths.append(filepath)\n",
    "        \n",
    "list_test = [filepath for filepath in list_paths if \"test/\" in filepath]\n",
    "index = [os.path.basename(filepath) for filepath in list_test]\n",
    "\n",
    "X_test = np.array([read_and_resize(filepath) for filepath in list_test])\n",
    "\n",
    "############# Generation complete ################\n",
    "\n",
    "file_path = 'best.hdf5'\n",
    "model.load_weights(file_path)\n",
    "\n",
    "predicts = model.predict(X_test)\n",
    "predicts = np.argmax(predicts, axis=1)\n",
    "predicts = [label_index[p] for p in predicts]\n",
    "\n",
    "df = pd.DataFrame(columns=['fname', 'camera'])\n",
    "df['fname'] = index\n",
    "df['camera'] = predicts\n",
    "df.to_csv(\"sub.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
