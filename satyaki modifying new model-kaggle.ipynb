{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69b2ebb2-71df-4bdb-9156-b3ca0d19478f",
    "_uuid": "0df9e934921cd91b0358b5752afe56ede37f3e1b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from random import shuffle\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de580565-a3b5-4b0c-b6e3-3fbd4bfd0b78",
    "_uuid": "0fab7b111545360cebd3388b6ac8030a5a1150e8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_paths = []\n",
    "for subdir, dirs, files in os.walk(\"../input\"):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath = subdir + os.sep + file\n",
    "        list_paths.append(filepath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ad2510c7-6cdc-4683-bdda-44de6e7b59e2",
    "_uuid": "cd6a34a8b8ac61560d75d06512e721e53706e67a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_test = [filepath for filepath in list_paths if \"test/\" in filepath]\n",
    "\n",
    "index = [os.path.basename(filepath) for filepath in list_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "16c8699d-2222-4de0-a297-0f1d9e77445b",
    "_uuid": "d7273fd6ff53762125d5248a243d1012ff3ce9c1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = list(set([os.path.dirname(filepath).split(os.sep)[-1] for filepath in list_paths if \"train\" in filepath]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "850c5764-e009-46ce-9080-96e0f47ffbc6",
    "_uuid": "56ac73fe15195693a58a8171c64dffadadec31f8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = ['Sony-NEX-7',\n",
    " 'Motorola-X',\n",
    " 'HTC-1-M7',\n",
    " 'Samsung-Galaxy-Note3',\n",
    " 'Motorola-Droid-Maxx',\n",
    " 'iPhone-4s',\n",
    " 'iPhone-6',\n",
    " 'LG-Nexus-5x',\n",
    " 'Samsung-Galaxy-S4',\n",
    " 'Motorola-Nexus-6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "abacfb3c-10df-4f19-b032-2b3fb06c48dc",
    "_uuid": "a2a733db5a2250c7daacd3e5bb12794d38c78a8c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_from_path(filepath):\n",
    "    return os.path.dirname(filepath).split(os.sep)[-1]\n",
    "\n",
    "def read_and_resize(filepath):\n",
    "    im_array = np.array(Image.open((filepath)), dtype=\"uint8\")\n",
    "    pil_im = Image.fromarray(im_array)\n",
    "    new_array = np.array(pil_im.resize((256,256)))\n",
    "    return new_array/255\n",
    "\n",
    "def label_transform(labels):\n",
    "    labels = pd.get_dummies(pd.Series(labels))\n",
    "    label_index = labels.columns.values\n",
    "\n",
    "    return labels, label_index\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2ec89065-7057-4557-8d19-57c61799f904",
    "_kg_hide-output": true,
    "_uuid": "d601bdbd9ee568b31ed0c3ac8a1ccafbabacca7f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.array([read_and_resize(filepath) for filepath in list_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d49ad9f6-aa39-4de4-9e3f-34d7e9a3cd44",
    "_uuid": "37c83c31f7619f00ae1e47d29cdf0b9d2517de7b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [get_class_from_path(filepath) for filepath in list_train]\n",
    "y, label_index = label_transform(labels)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b8ffb7b2-4b03-409b-ba7b-1208cb000395",
    "_uuid": "d784c240abe66ef95450913717748778cd0996bb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, time, datetime\n",
    "import numpy as np\n",
    "\n",
    "sd = 7\n",
    "np.random.seed(sd)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(sd)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,GlobalMaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, Nadam, Adamax\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "npzfile = 'E:/IP/Labels/labels.npz'\n",
    "\n",
    "dataset =  np.load(npzfile)\n",
    "x_train = dataset['X_train']\n",
    "y_train = dataset['Y_train']\n",
    "\n",
    "########### PARAMETERS LIST START############\n",
    "\n",
    "#kernel_initializer = initializers.glorot_normal(seed=sd) can be tested instead of he_normal(seed=sd)\n",
    "num_classes = 10\n",
    "eps = 1e-6\n",
    "dropout_rate_layers = 0.2\n",
    "dropout_rate_dense = 0.1\n",
    "maxpool_size = 2\n",
    "kernel_size = 5\n",
    "stride1 = 1\n",
    "stride2 = 1\n",
    "stride3 = 1\n",
    "stride4 = 1\n",
    "\n",
    "learning_rate = 0.003\n",
    "lr_decay = 0.0\n",
    "\n",
    "bias = False\n",
    "val_split = 0.3 # Valid split = (NumOftestImageDataFromKaggle / totalNumOfImages)*100% = (2640/20k)*100 = 13.2%\n",
    "batch_size = 32\n",
    "nepoch = 15\n",
    "\n",
    "l2reg_conv = 0.001 # for kernel_regularizer at conv layers\n",
    "l2reg_dense = 0.001 # for kernel_regularizer at dense layers\n",
    "\n",
    "#maxnorm = 1000 #for kernel_constraint at conv layers\n",
    "#moment = 0.99 #for batchNormalization => => add  momentum=moment at every BatchNormalization\n",
    "#run_idx = 11\n",
    "pad_type = 'valid' # test using 'same'\n",
    "\n",
    "# ReduceLROnPlateau params\n",
    "reducelr_factor = 0.2\n",
    "reducelr_patience = 3\n",
    "cooldown = 0 #number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "\n",
    "########### PARAMETERS LIST END ############\n",
    "\n",
    "# CNN MODEL STARTS!\n",
    "\n",
    "def CNN_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #layer 1\n",
    "    model.add(Conv2D(32, kernel_size=kernel_size-1, \n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                #kernel_constraint=max_norm(maxnorm),\n",
    "                use_bias=bias,\n",
    "                input_shape=(256,256,3))\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "    #batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    #layer 2\n",
    "    model.add(Conv2D(64,kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                #kernel_constraint=max_norm(maxnorm),\n",
    "                use_bias=bias )\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "#batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    \n",
    "    #layer 3\n",
    "    model.add(Conv2D(128,kernel_size=kernel_size, \n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                #kernel_constraint=max_norm(maxnorm),\n",
    "                use_bias=bias )\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "    #batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    #layer 4\n",
    "    model.add(Conv2D(256,kernel_size=kernel_size, \n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                #kernel_constraint=max_norm(maxnorm),\n",
    "                use_bias=bias)\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "    #batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    # now the fully connected layers!\n",
    "    \n",
    "    # DENSE layer 1\n",
    "    model.add(Dense(256, activation='relu',\n",
    "                   use_bias=bias,\n",
    "                   kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                   kernel_regularizer=regularizers.l2(l2reg_dense),\n",
    "                   #kernel_constraint=max_norm(maxnorm)\n",
    "                  ))\n",
    "    #model.add(Dropout(rate=dropout_rate_dense, seed=sd))\n",
    "    \n",
    "    # DENSE layer 2\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # types of OPTIMIZERS available\n",
    "    adam = Adam(lr=learning_rate,decay=lr_decay)\n",
    "    #sgd = SGD(lr=learning_rate, decay=lr_decay, momentum=moment, nesterov=True)\n",
    "    #rmsprop = RMSprop(lr=learning_rate)\n",
    "    #adamax = Adamax(lr=learning_rate)\n",
    "    #nadam = Nadam(lr=learning_rate)\n",
    "    \n",
    "    # types of LOSS FUNCTIONS available\n",
    "    ctg_crossent = 'categorical_crossentropy'\n",
    "    #sparse_ctg_crossent = 'sparse_categorical_crossentropy'\n",
    "    #mse = 'mean_squared_error'\n",
    "    \n",
    "    model.compile(loss=ctg_crossent , optimizer=adam , metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "file_path=\"weights.best.hdf5\"\n",
    "\n",
    "#saving weights as .hdf5 file. \n",
    "weights = model.get_weights()\n",
    "print(\"Weights: \", weights)\n",
    "\n",
    "#checkpoints!\n",
    "modelcheck=ModelCheckpoint(filepath=checkpoint_name,monitor='val_catagorical_acc', verbose=1,save_best_only=False,mode='max')\n",
    "tensorbd=TensorBoard(log_dir='./logs/'+log_name,batch_size=batch_size,write_images=True)\n",
    "csv_logger = CSVLogger('./logs/training_'+log_name+'.log',separator=',', append=True )\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reducelr_factor, patience=reducelr_patience, cooldown=cooldown, min_lr=0.0001, verbose=1, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, mode='min')\n",
    "\n",
    "checkpoints = [modelcheck, tensorbd, csv_logger, reduce_lr, early_stop]\n",
    "\n",
    "#fitting model to for validation\n",
    "training = model.fit(x, y_train, \n",
    "                     validation_split = val_split,\n",
    "                     epochs=nepoch, \n",
    "                     batch_size=batch_size, \n",
    "                     verbose=2, #verbose=1 can result in slower training time.\n",
    "                     shuffle=True,\n",
    "                     callbacks = checkpoints)\n",
    "print(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PLOTTING VAL_ACC AND lOSS USING MATPLOTLIB #####\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# show the loss and accuracy\n",
    "loss = training.history['loss']\n",
    "val_loss = training.history['val_loss']\n",
    "acc = training.history['acc']\n",
    "val_acc = training.history['val_acc']\n",
    "\n",
    "# loss plot\n",
    "tra = plt.plot(loss)\n",
    "val = plt.plot(val_loss, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# accuracy plot\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c61441e5-fbd3-4aba-a90a-e0083b92a944",
    "_uuid": "412a548f87be342504eb28d8eed716935ec5c346",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "\n",
    "predicts = model.predict(X_test)\n",
    "predicts = np.argmax(predicts, axis=1)\n",
    "predicts = [label_index[p] for p in predicts]\n",
    "\n",
    "df = pd.DataFrame(columns=['fname', 'camera'])\n",
    "df['fname'] = index\n",
    "df['camera'] = predicts\n",
    "df.to_csv(\"sub.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
