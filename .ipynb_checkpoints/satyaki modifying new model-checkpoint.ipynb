{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/HTC-1-M7/*.jpg'\n",
    "\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '1'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'w')\n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/iPhone-4s/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '2'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/iPhone-6/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '3'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')\n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/LG Nexus 5x/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '4'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Motorola Droid Maxx/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '5'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Motorola Nexus 6/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '6'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Motorola X/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '7'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Samsung Galaxy Note 4/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '8'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Samsung Galaxy S4/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '9'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()\n",
    "\n",
    "\n",
    "readpath = 'E:/IP/dataset 256/Sony Nex 7/*.jpg'\n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "objectClass = '10'\n",
    "\n",
    "images = glob.glob(readpath)\n",
    "labelfile = open(labels,'a')  \n",
    "\n",
    "for image in images:\n",
    "    labelfile.write(image+','+objectClass+'\\n')\n",
    "\n",
    "labelfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for shuffle the image \n",
    "import random\n",
    "# read the csv file \n",
    "labels = 'E:/IP/Labels/labels.csv'\n",
    "#create a new csv file as shuffled_labels.csv where the image address will be shuffled\n",
    "shuffled_labels = 'E:/IP/Labels/shuffled_labels.csv'\n",
    "\n",
    "labelfile = open(labels, \"r\")\n",
    "lines = labelfile.readlines()\n",
    "labelfile.close()\n",
    "random.shuffle(lines)\n",
    "# create shuffle csv\n",
    "shufflefile = open(shuffled_labels, \"w\") \n",
    "shufflefile.writelines(lines)\n",
    "shufflefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "labels = 'E:/IP/Labels/shuffled_labels.csv'\n",
    "npzfile = 'E:/IP/Labels/labels.npz'\n",
    "#create npz hiden  file \n",
    "df = pandas.read_csv(labels)\n",
    "\n",
    "rows = df.iterrows()\n",
    "\n",
    "X_temp = []\n",
    "Y_temp = []\n",
    "\n",
    "for row in rows:\n",
    "    image = row[1][0]\n",
    "    img = cv2.imread(image)\n",
    "    #npimg = np.asarray(img)\n",
    "    #npimg = npimg/255\n",
    "    (b, g, r)=cv2.split(img)\n",
    "    img=cv2.merge([r,g,b])\n",
    "    imageClass = row[1][1]\n",
    "    X_temp.append(img)\n",
    "    Y_temp.append(imageClass)\n",
    "\n",
    "       \n",
    "# one hot encoding to represent one num as a array like 2= [0 1 0] in a 3 class cnn model     \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_temp)\n",
    "encoded_Y = encoder.transform(Y_temp)\n",
    "Y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "np.savez(npzfile, X_train=X_temp,Y_train=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, time, datetime\n",
    "import numpy as np\n",
    "\n",
    "sd = 7\n",
    "np.random.seed(sd)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(sd)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, Nadam, Adamax\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(),'saved_models_keras')\n",
    "model_name = 'SPcup_trained_model.h5'\n",
    "\n",
    "\n",
    "npzfile = 'E:/IP/Labels/labels.npz'\n",
    "\n",
    "dataset =  np.load(npzfile)\n",
    "x_train = dataset['X_train']\n",
    "y_train = dataset['Y_train']\n",
    "\n",
    "# function for writing the model.summary() to a text file.\n",
    "def myprint(s):\n",
    "    with open('\\modelsummary\\model_summary'+'_' + str(run_idx) + '.txt','w+') as f:\n",
    "        print(s, file=f)\n",
    "\n",
    "#x = float(x_train)/255.0\n",
    "x /= np.max(x_train)\n",
    "\n",
    "########### PARAMETERS LIST START############\n",
    "\n",
    "#kernel_initializer = initializers.glorot_normal(seed=sd) can be tested instead of he_normal(seed=sd)\n",
    "num_classes = 10\n",
    "eps = 1e-6\n",
    "dropout_rate_layers = 0.2\n",
    "dropout_rate_dense = 0.05\n",
    "maxpool_size = 2\n",
    "kernel_size = 5\n",
    "stride1 = 1\n",
    "stride2 = 1\n",
    "stride3 = 1\n",
    "stride4 = 1\n",
    "\n",
    "learning_rate = 0.001\n",
    "lr_decay = 0.0\n",
    "\n",
    "bias = False\n",
    "val_split = 0.15 # Valid split = (NumOftestImageDataFromKaggle / totalNumOfImages)*100% = (2640/20k)*100 = 13.2%\n",
    "batch_size = 32\n",
    "nepoch = 20\n",
    "\n",
    "l2reg_conv = 0.001 # for kernel_regularizer at conv layers\n",
    "l2reg_dense = 0.005 # for kernel_regularizer at dense layers\n",
    "\n",
    "maxnorm = 1000 #for kernel_constraint at conv layers\n",
    "moment = 0.99 #for batchNormalization\n",
    "run_idx = 11\n",
    "pad_type = 'valid' # test using 'same'\n",
    "\n",
    "# ReduceLROnPlateau params\n",
    "reducelr_factor = 0.2\n",
    "reducelr_patience = 3\n",
    "cooldown = 0 #number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "\n",
    "########### PARAMETERS LIST END ############\n",
    "\n",
    "# CNN MODEL STARTS!\n",
    "\n",
    "def CNN_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #layer 1\n",
    "    model.add(Conv2D(32, kernel_size=kernel_size-1, \n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=bias,\n",
    "                input_shape=(256,256,3))\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "    #batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1, momentum=moment))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    #layer 2\n",
    "    model.add(Conv2D(64,kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=bias )\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "#batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1, momentum=moment))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    \n",
    "    #layer 3\n",
    "    model.add(Conv2D(128,kernel_size=kernel_size, \n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=bias )\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "    #batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1, momentum=moment))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    #layer 4\n",
    "    model.add(Conv2D(256,kernel_size=kernel_size, \n",
    "                strides=stride,\n",
    "                padding=pad_type,\n",
    "                kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                data_format=\"channels_last\", #(batch, height, width, channels)\n",
    "                kernel_regularizer=regularizers.l2(l2reg_conv), #or kernel_regularizer=None.\n",
    "                kernel_constraint=max_norm(maxnorm)\n",
    "                use_bias=bias)\n",
    "             )\n",
    "    model.add(Activation('relu'))\n",
    "    #batchNormalization AFTER relu is good => https://arxiv.org/abs/1511.06422\n",
    "    model.add(BatchNormalization(epsilon=eps, axis=-1, momentum=moment))\n",
    "    model.add(Dropout(rate=dropout_rate_layers, seed=sd))\n",
    "    model.add(MaxPooling2D(pool_size=maxpool_size,\n",
    "                         # strides=maxpool_size,\n",
    "                         data_format=\"channels_last\"))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    # now the fully connected layers!\n",
    "    \n",
    "    # DENSE layer 1\n",
    "    model.add(Dense(256, activation='relu',\n",
    "                   use_bias=bias,\n",
    "                   kernel_initializer=initializers.he_normal(seed=sd),\n",
    "                   kernel_regularizer=regularizers.l2(l2reg_dense),\n",
    "                   kernel_constraint=max_norm(maxnorm)\n",
    "                  ))\n",
    "    #model.add(Dropout(rate=dropout_rate_dense, seed=sd))\n",
    "    \n",
    "    # DENSE layer 2\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # types of OPTIMIZERS available\n",
    "    adam = Adam(lr=learning_rate,decay=lr_decay)\n",
    "    sgd = SGD(lr=learning_rate, decay=lr_decay, momentum=moment, nesterov=True)\n",
    "    rmsprop = RMSprop(lr=learning_rate)\n",
    "    adamax = Adamax(lr=learning_rate)\n",
    "    nadam = Nadam(lr=learning_rate)\n",
    "    \n",
    "    # types of LOSS FUNCTIONS available\n",
    "    ctg_crossent = 'categorical_crossentropy'\n",
    "    sparse_ctg_crossent = 'sparse_categorical_crossentropy'\n",
    "    mse = 'mean_squared_error'\n",
    "    \n",
    "    model.compile(loss=catag_crossent , optimizer=adam , metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "# CNN MODEL ENDS!\n",
    "\n",
    "model = CNN_model()\n",
    "\n",
    "model.summary(model.summary(print_fn=myprint))\n",
    "\n",
    "# check will store the best model weight which will monitor the validation accuracy not train accuracy\n",
    "file_path = 'best' + '_' + str(run_idx) + '.hdf5'\n",
    "\n",
    "#saving weights as .hdf5 file. \n",
    "weights = model.get_weights()\n",
    "print(\"Weights: \", weights)\n",
    "model.save_weights(file_path) ####################### ????????????????\n",
    "\n",
    "#setting up checkpoint save directory/ log names\n",
    "checkpoint_path=os.path.join(os.path.join(os.getcwd(),'tmp'),str(date.today()))\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_name=checkpoint_path+'/'+str(run_idx)+'weights.{epoch:02d}-{val_acc:.4f}.hdf5'\n",
    "log_name='logs'+ '_' + str(date.today()) + '_' + str(run_idx)\n",
    "\n",
    "#checkpoints!\n",
    "modelcheck=ModelCheckpoint(filepath=checkpoint_name,monitor='val_catagorical_acc', verbose=1,save_best_only=False,mode='max')\n",
    "tensorbd=TensorBoard(log_dir='./logs/'+log_name,batch_size=batch_size,write_images=True)\n",
    "csv_logger = CSVLogger('./logs/training_'+log_name+'.log',separator=',', append=True )\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reducelr_factor, patience=reducelr_patience, cooldown=cooldown, min_lr=0.0001, verbose=1, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, mode='min')\n",
    "\n",
    "checkpoints = [modelcheck, tensorbd, csv_logger, reduce_lr, early_stop]\n",
    "\n",
    "#fitting model to for validation\n",
    "training = model.fit(x, y_train, \n",
    "                     validation_split = val_split,\n",
    "                     epochs=nepoch, \n",
    "                     batch_size=batch_size, \n",
    "                     verbose=2, #verbose=1 can result in slower training time.\n",
    "                     shuffle=True,\n",
    "                     callbacks = checkpoints)\n",
    "print(training)\n",
    "\n",
    "#plot_model() uses pyDot and GraphViz\n",
    "plot_model(model, to_file='model.png')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PLOTTING VAL_ACC AND lOSS USING MATPLOTLIB #####\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# show the loss and accuracy\n",
    "loss = training.history['loss']\n",
    "val_loss = training.history['val_loss']\n",
    "acc = training.history['acc']\n",
    "val_acc = training.history['val_acc']\n",
    "\n",
    "# loss plot\n",
    "tra = plt.plot(loss)\n",
    "val = plt.plot(val_loss, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# accuracy plot\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc, 'r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOHEL'S FIRST CODE FOR TESTING.\n",
    "\n",
    "#predict the class\n",
    "y = model.predict_classes(X)\n",
    "# y is a one dim array we have to convert it to a classs num like 3,4 \n",
    "classno = np.ndarray.tolist(y)\n",
    "\n",
    "dict = {0: 'LG', 1:'moto maxx', 2: 'sony,}\n",
    "objectClass = dict[classno[0]]\n",
    "print(objectClass)\n",
    "print(y)\n",
    "        \n",
    "#yPred = model.predict_classes(x_valid,batch_size=32,verbose=1)\n",
    "\n",
    "#np.savetxt('mnist_output.csv', np.c_[range(1,len(yPred)+1),yPred], delimiter=',', header = 'ImageId,Label', comments = '', fmt='%d')\n",
    "\n",
    "#font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#cv2.putText(img, objectClass,(50,50), font, 2, (200,255,0), 5, cv2.LINE_AA)\n",
    "#cv2.imshow('Prediction',img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOT FROM THIS LINK: https://goo.gl/kRPp9p\n",
    "\n",
    "# load the best model\n",
    "model = load_model(file_path)\n",
    "\n",
    "# load test_data\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# prediction\n",
    "pred = model.predict(test_features, batch_size=16, \n",
    "                       verbose=1)\n",
    "\n",
    "# convert predicions from categorical back to 0...9 digits\n",
    "pred_digits = np.argmax(pred, axis=1)\n",
    "\n",
    "submission = pandas.DataFrame({'Label': pred_digits})\n",
    "submission.index += 1\n",
    "submission.index.name = \"ImageId\"\n",
    "\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOT FROM THIS LINK: https://goo.gl/wk2WwP\n",
    "\n",
    "############## LOAD MODEL ########################\n",
    "net = load_model(file_path)\n",
    "\n",
    "############## PREDICT ########################\n",
    "\n",
    "test_imgs = os.listdir(test_image_path)\n",
    "predict_df = pd.DataFrame(index=np.arange(0, nb_rows), columns=classes)\n",
    "\n",
    "count = 0\n",
    "for img in test_imgs:\n",
    "    img = image.load_img(test+image_path+img, target_size=(256, 256))\n",
    "    x = image.img_to_array(img)/255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    #Actual prediction\n",
    "    preds = net.predict_proba(x)[0]\n",
    "    predict_df.loc[count] = preds\n",
    "    count += 1\n",
    "    \n",
    "img_name_df = pd.read_csv(\"../submission/sample_submission_stg1.csv\")\n",
    "img_name_df = img_name_df[nm_img]\n",
    "\n",
    "predict_df.insert(0,nm_img,img_name_df)\n",
    "\n",
    "predict_df.to_csv('nature_predicts.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KAGGLE SUBMISSION TYPE.\n",
    "\n",
    "############## generating X_test ##############\n",
    "import os\n",
    "\n",
    "list_paths = []\n",
    "for subdir, dirs, files in os.walk(\"E:\\IP\"):\n",
    "    for file in files:\n",
    "        #print(os.path.join(subdir, file))\n",
    "        filepath = subdir + os.sep + file\n",
    "        list_paths.append(filepath)\n",
    "        \n",
    "list_test = [filepath for filepath in list_paths if \"test/\" in filepath]\n",
    "index = [os.path.basename(filepath) for filepath in list_test]\n",
    "\n",
    "X_test = np.array([read_and_resize(filepath) for filepath in list_test])\n",
    "\n",
    "############# Generation complete ################\n",
    "\n",
    "file_path = 'best.hdf5'\n",
    "model.load_weights(file_path)\n",
    "\n",
    "predicts = model.predict(X_test)\n",
    "predicts = np.argmax(predicts, axis=1)\n",
    "predicts = [label_index[p] for p in predicts]\n",
    "\n",
    "df = pd.DataFrame(columns=['fname', 'camera'])\n",
    "df['fname'] = index\n",
    "df['camera'] = predicts\n",
    "df.to_csv(\"sub.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
